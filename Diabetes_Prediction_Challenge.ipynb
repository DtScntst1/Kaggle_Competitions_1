{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Engineering features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-01 15:11:19,442] A new study created in memory with name: no-name-1002ce73-6626-4a91-8cf1-e5522717d345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n",
      "Running Optuna for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-01 15:12:34,470] Trial 0 finished with value: 0.7223411035016231 and parameters: {'n_estimators': 1025, 'learning_rate': 0.06253537504208502, 'max_depth': 6, 'subsample': 0.526753034214585, 'colsample_bytree': 0.7678852301155152, 'reg_alpha': 0.04279643621401936, 'reg_lambda': 1.6656147325727316e-05}. Best is trial 0 with value: 0.7223411035016231.\n",
      "[I 2025-12-01 15:15:39,001] Trial 1 finished with value: 0.7231782094163469 and parameters: {'n_estimators': 1293, 'learning_rate': 0.007809163200743826, 'max_depth': 9, 'subsample': 0.8097868161803469, 'colsample_bytree': 0.8423420508509891, 'reg_alpha': 0.12838633439799393, 'reg_lambda': 9.879757534347639e-06}. Best is trial 1 with value: 0.7231782094163469.\n",
      "[I 2025-12-01 15:17:25,141] Trial 2 finished with value: 0.7198921268582076 and parameters: {'n_estimators': 1397, 'learning_rate': 0.015396566747302819, 'max_depth': 4, 'subsample': 0.9851625852879, 'colsample_bytree': 0.8245727807355291, 'reg_alpha': 8.843304253457662e-08, 'reg_lambda': 0.02175732958666038}. Best is trial 1 with value: 0.7231782094163469.\n",
      "[I 2025-12-01 15:19:20,841] Trial 3 finished with value: 0.7236717030748472 and parameters: {'n_estimators': 1469, 'learning_rate': 0.03080967258743691, 'max_depth': 4, 'subsample': 0.5433373121714516, 'colsample_bytree': 0.8665989101642401, 'reg_alpha': 3.6335721079224655e-08, 'reg_lambda': 2.561486060976088}. Best is trial 3 with value: 0.7236717030748472.\n",
      "[I 2025-12-01 15:21:14,148] Trial 4 finished with value: 0.7101483552493738 and parameters: {'n_estimators': 1384, 'learning_rate': 0.009516748623487161, 'max_depth': 3, 'subsample': 0.7061608274900992, 'colsample_bytree': 0.7120249893054162, 'reg_alpha': 0.0056445621606325816, 'reg_lambda': 0.0009471102919640135}. Best is trial 3 with value: 0.7236717030748472.\n",
      "[I 2025-12-01 15:22:43,054] Trial 5 finished with value: 0.7227597308604112 and parameters: {'n_estimators': 556, 'learning_rate': 0.024476914319806073, 'max_depth': 10, 'subsample': 0.613006170114079, 'colsample_bytree': 0.6172232473001846, 'reg_alpha': 0.00844670746932911, 'reg_lambda': 0.02402125498732145}. Best is trial 3 with value: 0.7236717030748472.\n",
      "[I 2025-12-01 15:27:30,727] Trial 6 finished with value: 0.719792541679959 and parameters: {'n_estimators': 1921, 'learning_rate': 0.02443429897985827, 'max_depth': 9, 'subsample': 0.5770202411706353, 'colsample_bytree': 0.9414829906286026, 'reg_alpha': 1.8094622190570355e-07, 'reg_lambda': 0.0008074983300453609}. Best is trial 3 with value: 0.7236717030748472.\n",
      "[I 2025-12-01 15:31:21,812] Trial 7 finished with value: 0.70917284392571 and parameters: {'n_estimators': 1749, 'learning_rate': 0.08390953645541081, 'max_depth': 8, 'subsample': 0.7666497615753123, 'colsample_bytree': 0.6923882805974345, 'reg_alpha': 0.5551596730358916, 'reg_lambda': 0.008167212822632296}. Best is trial 3 with value: 0.7236717030748472.\n",
      "[I 2025-12-01 15:33:15,327] Trial 8 finished with value: 0.7256795665282835 and parameters: {'n_estimators': 1368, 'learning_rate': 0.059344162506351314, 'max_depth': 4, 'subsample': 0.8621772098416591, 'colsample_bytree': 0.5172755423849738, 'reg_alpha': 0.00024974538350067653, 'reg_lambda': 7.478363200208106}. Best is trial 8 with value: 0.7256795665282835.\n",
      "[I 2025-12-01 15:34:35,675] Trial 9 finished with value: 0.7221927314587445 and parameters: {'n_estimators': 852, 'learning_rate': 0.07961300459725505, 'max_depth': 6, 'subsample': 0.5821706059240336, 'colsample_bytree': 0.6477516579724916, 'reg_alpha': 5.224877484825298e-06, 'reg_lambda': 0.10182790552722593}. Best is trial 8 with value: 0.7256795665282835.\n",
      "[I 2025-12-01 15:36:37,832] Trial 10 finished with value: 0.7256207158865631 and parameters: {'n_estimators': 1630, 'learning_rate': 0.04395210948579147, 'max_depth': 5, 'subsample': 0.9237123258688824, 'colsample_bytree': 0.52137361733498, 'reg_alpha': 7.878843693639035e-05, 'reg_lambda': 8.714717282976107e-08}. Best is trial 8 with value: 0.7256795665282835.\n",
      "[I 2025-12-01 15:38:50,991] Trial 11 finished with value: 0.7255625619504643 and parameters: {'n_estimators': 1715, 'learning_rate': 0.04735060012512242, 'max_depth': 5, 'subsample': 0.9223204498140227, 'colsample_bytree': 0.5081587531792351, 'reg_alpha': 7.061835880141109e-05, 'reg_lambda': 1.1666809127648788e-08}. Best is trial 8 with value: 0.7256795665282835.\n",
      "[I 2025-12-01 15:40:45,057] Trial 12 finished with value: 0.7237327568834627 and parameters: {'n_estimators': 1623, 'learning_rate': 0.0421645343677121, 'max_depth': 3, 'subsample': 0.8709223757437762, 'colsample_bytree': 0.5091875081245476, 'reg_alpha': 0.00010139343789007683, 'reg_lambda': 4.580172346428533e-07}. Best is trial 8 with value: 0.7256795665282835.\n",
      "[I 2025-12-01 15:42:20,898] Trial 13 finished with value: 0.7254124657376769 and parameters: {'n_estimators': 1175, 'learning_rate': 0.04197049248112456, 'max_depth': 5, 'subsample': 0.8846765081443752, 'colsample_bytree': 0.5912605785046938, 'reg_alpha': 4.8574395085412795e-06, 'reg_lambda': 4.710547796714284}. Best is trial 8 with value: 0.7256795665282835.\n",
      "[I 2025-12-01 15:44:35,640] Trial 14 finished with value: 0.7165602014318142 and parameters: {'n_estimators': 1561, 'learning_rate': 0.09706680312457831, 'max_depth': 7, 'subsample': 0.9712522185577631, 'colsample_bytree': 0.5644854292543955, 'reg_alpha': 0.0008706438079840723, 'reg_lambda': 1.2519312124978378e-08}. Best is trial 8 with value: 0.7256795665282835.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGB params: {'n_estimators': 1368, 'learning_rate': 0.059344162506351314, 'max_depth': 4, 'subsample': 0.8621772098416591, 'colsample_bytree': 0.5172755423849738, 'reg_alpha': 0.00024974538350067653, 'reg_lambda': 7.478363200208106}\n",
      "XGBoost CV AUC: 0.72653\n",
      "\n",
      "Training LightGBM...\n",
      "LightGBM CV AUC: 0.72616\n",
      "\n",
      "Training CatBoost...\n",
      "CatBoost CV AUC: 0.72299\n",
      "\n",
      "Optimizing Ensemble Weights...\n",
      "Best Weights:\n",
      "  xgb: 0.3393\n",
      "  lgb: 0.3306\n",
      "  cat: 0.3301\n",
      "\n",
      "Final Ensemble CV AUC: 0.72611\n",
      "Saved: submission_advanced_optuna.csv\n"
     ]
    }
   ],
   "source": [
    "# DIABETES PREDICTION CHALLENGE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check for available libraries\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not installed.\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not installed.\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "    print(\"CatBoost not installed.\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not installed. Skipping hyperparameter tuning.\")\n",
    "\n",
    "def load_and_preprocess():\n",
    "    print(\"Loading data...\")\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    # Handle ID column\n",
    "    if \"id\" in train.columns:\n",
    "        train = train.drop(\"id\", axis=1)\n",
    "    if \"id\" in test.columns:\n",
    "        test_ids = test[\"id\"]\n",
    "        test = test.drop(\"id\", axis=1)\n",
    "    else:\n",
    "        test_ids = test.index\n",
    "\n",
    "    # Combine for processing\n",
    "    train[\"is_train\"] = 1\n",
    "    test[\"is_train\"] = 0\n",
    "    df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "    # Feature Engineering\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # 1. Winsorization (Clip outliers)\n",
    "    # Clipping extreme values in cholesterol and triglycerides which can confuse models\n",
    "    for col in [\"cholesterol_total\", \"ldl_cholesterol\", \"triglycerides\"]:\n",
    "        upper_limit = df[col].quantile(0.99)\n",
    "        df[col] = df[col].clip(upper=upper_limit)\n",
    "\n",
    "    # 2. Blood Pressure Components\n",
    "    df[\"Pulse_Pressure\"] = df[\"systolic_bp\"] - df[\"diastolic_bp\"]\n",
    "    df[\"MAP\"] = df[\"diastolic_bp\"] + (1 / 3) * df[\"Pulse_Pressure\"]\n",
    "    \n",
    "    # 3. Cholesterol Ratios\n",
    "    df[\"hdl_cholesterol\"] = df[\"hdl_cholesterol\"].replace(0, 0.1)\n",
    "    df[\"Cholesterol_Ratio\"] = df[\"cholesterol_total\"] / df[\"hdl_cholesterol\"]\n",
    "    df[\"LDL_HDL_Ratio\"] = df[\"ldl_cholesterol\"] / df[\"hdl_cholesterol\"]\n",
    "    df[\"Trig_HDL_Ratio\"] = df[\"triglycerides\"] / df[\"hdl_cholesterol\"]\n",
    "    df[\"Non_HDL\"] = df[\"cholesterol_total\"] - df[\"hdl_cholesterol\"]\n",
    "    \n",
    "    # 4. Interaction Terms\n",
    "    df[\"BMI_Age\"] = df[\"bmi\"] * df[\"age\"]\n",
    "    df[\"BP_Product\"] = df[\"systolic_bp\"] * df[\"diastolic_bp\"]\n",
    "    \n",
    "    # 5. Binning\n",
    "    # BMI Categories\n",
    "    df['BMI_Class'] = pd.cut(df['bmi'], \n",
    "                             bins=[-np.inf, 18.5, 24.9, 29.9, np.inf], \n",
    "                             labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "    \n",
    "    # Blood Pressure Categories\n",
    "    def categorize_bp(row):\n",
    "        sys = row['systolic_bp']\n",
    "        dia = row['diastolic_bp']\n",
    "        if sys < 120 and dia < 80:\n",
    "            return 'Normal'\n",
    "        elif 120 <= sys < 130 and dia < 80:\n",
    "            return 'Elevated'\n",
    "        elif 130 <= sys < 140 or 80 <= dia < 90:\n",
    "            return 'Stage1'\n",
    "        else:\n",
    "            return 'Stage2'\n",
    "    df['BP_Class'] = df.apply(categorize_bp, axis=1)\n",
    "    \n",
    "    # 6. Log Transforms\n",
    "    skewed_cols = [\"triglycerides\", \"ldl_cholesterol\", \"cholesterol_total\", \"bmi\"]\n",
    "    for col in skewed_cols:\n",
    "        if col in df.columns:\n",
    "            df[f\"Log_{col}\"] = np.log1p(df[col])\n",
    "\n",
    "    # Encoding & Cleaning\n",
    "    train_df = df[df[\"is_train\"] == 1].drop(\"is_train\", axis=1)\n",
    "    test_df = df[df[\"is_train\"] == 0].drop([\"is_train\", \"diagnosed_diabetes\"], axis=1)\n",
    "    \n",
    "    cat_cols = train_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    \n",
    "    if cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        for col in cat_cols:\n",
    "            train_df[col] = train_df[col].astype(str)\n",
    "            test_df[col] = test_df[col].astype(str)\n",
    "            \n",
    "            all_values = pd.concat([train_df[col], test_df[col]])\n",
    "            le.fit(all_values)\n",
    "            \n",
    "            train_df[col] = le.transform(train_df[col])\n",
    "            test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "    # Impute Missing Values\n",
    "    num_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols.remove(\"diagnosed_diabetes\")\n",
    "    \n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    train_df[num_cols] = imputer.fit_transform(train_df[num_cols])\n",
    "    test_df[num_cols] = imputer.transform(test_df[num_cols])\n",
    "\n",
    "    return train_df, test_df, test_ids\n",
    "\n",
    "def optimize_xgb(X, y, n_trials=20):\n",
    "    if not OPTUNA_AVAILABLE: return {}\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for tr, val in kf.split(X, y):\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X[tr], y.iloc[tr])\n",
    "            preds = model.predict_proba(X[val])[:, 1]\n",
    "            scores.append(roc_auc_score(y.iloc[val], preds))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    print(f\"Best XGB params: {study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def train_and_predict(train, test, test_ids, use_optuna=False):\n",
    "    X = train.drop(\"diagnosed_diabetes\", axis=1)\n",
    "    y = train[\"diagnosed_diabetes\"]\n",
    "\n",
    "    # Robust Scaling is better for outliers\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    oof_preds = {}\n",
    "    test_preds = {}\n",
    "    \n",
    "    # XGBoost\n",
    "    if XGB_AVAILABLE:\n",
    "        print(\"\\nTraining XGBoost...\")\n",
    "        if use_optuna and OPTUNA_AVAILABLE:\n",
    "            print(\"Running Optuna for XGBoost...\")\n",
    "            best_params = optimize_xgb(X_scaled, y, n_trials=15) # Small trials for speed\n",
    "            xgb_params = best_params\n",
    "            xgb_params.update({'objective': 'binary:logistic', 'eval_metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'tree_method': 'hist'})\n",
    "        else:\n",
    "            # Tuned manually for general cases\n",
    "            xgb_params = {\n",
    "                'n_estimators': 1200,\n",
    "                'learning_rate': 0.015,\n",
    "                'max_depth': 6,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.6,\n",
    "                'reg_alpha': 0.01,\n",
    "                'reg_lambda': 1.0,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'n_jobs': -1,\n",
    "                'random_state': 42,\n",
    "                'tree_method': 'hist'\n",
    "            }\n",
    "        \n",
    "        oof = np.zeros(len(X))\n",
    "        pred = np.zeros(len(test))\n",
    "        \n",
    "        for fold, (tr, val) in enumerate(kf.split(X_scaled, y)):\n",
    "            model = XGBClassifier(**xgb_params)\n",
    "            model.fit(X_scaled[tr], y.iloc[tr], eval_set=[(X_scaled[val], y.iloc[val])], verbose=False)\n",
    "            oof[val] = model.predict_proba(X_scaled[val])[:, 1]\n",
    "            pred += model.predict_proba(test_scaled)[:, 1] / kf.get_n_splits()\n",
    "            \n",
    "        print(f\"XGBoost CV AUC: {roc_auc_score(y, oof):.5f}\")\n",
    "        oof_preds['xgb'] = oof\n",
    "        test_preds['xgb'] = pred\n",
    "\n",
    "    # LightGBM\n",
    "    if LGBM_AVAILABLE:\n",
    "        print(\"\\nTraining LightGBM...\")\n",
    "        lgb_params = {\n",
    "            'n_estimators': 1500,\n",
    "            'learning_rate': 0.02,\n",
    "            'num_leaves': 40,\n",
    "            'max_depth': -1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        oof = np.zeros(len(X))\n",
    "        pred = np.zeros(len(test))\n",
    "        \n",
    "        for fold, (tr, val) in enumerate(kf.split(X_scaled, y)):\n",
    "            model = LGBMClassifier(**lgb_params)\n",
    "            model.fit(X_scaled[tr], y.iloc[tr])\n",
    "            oof[val] = model.predict_proba(X_scaled[val])[:, 1]\n",
    "            pred += model.predict_proba(test_scaled)[:, 1] / kf.get_n_splits()\n",
    "            \n",
    "        print(f\"LightGBM CV AUC: {roc_auc_score(y, oof):.5f}\")\n",
    "        oof_preds['lgb'] = oof\n",
    "        test_preds['lgb'] = pred\n",
    "\n",
    "    # CatBoost\n",
    "    if CATBOOST_AVAILABLE:\n",
    "        print(\"\\nTraining CatBoost...\")\n",
    "        cat_params = {\n",
    "            'iterations': 1500,\n",
    "            'learning_rate': 0.02,\n",
    "            'depth': 7,\n",
    "            'l2_leaf_reg': 5,\n",
    "            'loss_function': 'Logloss',\n",
    "            'eval_metric': 'AUC',\n",
    "            'random_seed': 42,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        \n",
    "        oof = np.zeros(len(X))\n",
    "        pred = np.zeros(len(test))\n",
    "        \n",
    "        for fold, (tr, val) in enumerate(kf.split(X_scaled, y)):\n",
    "            model = CatBoostClassifier(**cat_params)\n",
    "            model.fit(X_scaled[tr], y.iloc[tr])\n",
    "            oof[val] = model.predict_proba(X_scaled[val])[:, 1]\n",
    "            pred += model.predict_proba(test_scaled)[:, 1] / kf.get_n_splits()\n",
    "            \n",
    "        print(f\"CatBoost CV AUC: {roc_auc_score(y, oof):.5f}\")\n",
    "        oof_preds['cat'] = oof\n",
    "        test_preds['cat'] = pred\n",
    "\n",
    "    # Weighted Ensemble Optimization\n",
    "    print(\"\\nOptimizing Ensemble Weights...\")\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    models = list(oof_preds.keys())\n",
    "    predictions = [oof_preds[m] for m in models]\n",
    "    \n",
    "    def loss_func(weights):\n",
    "        final_oof = np.zeros(len(y))\n",
    "        for i, w in enumerate(weights):\n",
    "            final_oof += w * predictions[i]\n",
    "        return -roc_auc_score(y, final_oof)\n",
    "    \n",
    "    init_weights = [1/len(models)] * len(models)\n",
    "    cons = ({'type': 'eq', 'fun': lambda w: 1 - sum(w)})\n",
    "    bounds = [(0, 1)] * len(models)\n",
    "    \n",
    "    res = minimize(loss_func, init_weights, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "    best_weights = res.x\n",
    "    \n",
    "    print(\"Best Weights:\")\n",
    "    for m, w in zip(models, best_weights):\n",
    "        print(f\"  {m}: {w:.4f}\")\n",
    "        \n",
    "    final_test_pred = np.zeros(len(test))\n",
    "    final_oof_pred = np.zeros(len(y))\n",
    "    \n",
    "    for i, m in enumerate(models):\n",
    "        final_test_pred += best_weights[i] * test_preds[m]\n",
    "        final_oof_pred += best_weights[i] * oof_preds[m]\n",
    "        \n",
    "    print(f\"\\nFinal Ensemble CV AUC: {roc_auc_score(y, final_oof_pred):.5f}\")\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_ids,\n",
    "        \"diagnosed_diabetes\": final_test_pred\n",
    "    })\n",
    "    submission.to_csv(\"submission_advanced_optuna.csv\", index=False)\n",
    "    print(\"Saved: submission_advanced_optuna.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Enable Optuna by setting use_optuna=True\n",
    "    # Note: Optuna takes time. For quick run, set to False.\n",
    "    USE_OPTUNA = True \n",
    "    \n",
    "    train_df, test_df, test_ids = load_and_preprocess()\n",
    "    train_and_predict(train_df, test_df, test_ids, use_optuna=USE_OPTUNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbWX2oGk60C2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9045607,
     "sourceId": 76727,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
